---
title: "ReadMe - Rose Blanche"
author: "Keith Lewis"
date: Sys.Date()
output: 
  html_document:
    toc: true
    toc_float: true
bibliography: refs/references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

df_all <- read.csv("data_derived/df_all.csv")
df_tab2 <- read.csv("data_derived/df_tab2.csv")
df_a <- read.csv("data_derived/df_a3.csv")
df_out <- read.csv("data_derived/FSA_output.csv")
df_sum_subset <- read.csv("data_derived/spc_example.csv")

library(ggplot2)
library(dplyr)
library(tidyr)
```


# Introduction
This file is simply a record of issues that I have had with the Rose Blanche analysis that was started by Kristin Loughlin and is a continuation of work begun by Dave Scruton. See ReadMe files from Pamehac and Seal Cove for additional info.

**See "ReadMe-CS-delta-methods" for general notes on the AMEC function (how it works and output), the CS Method, and Delta Method.  This files is for specifics of the RB analysis.


# Notes

Got a zip file from Kristin Loughlin with folders as follows:  
- Carle Strub Estimates  
- RB_depletion  
- a bunch of files which have been put in:  
    - data  
    - figs_Kristin  
    
I reorganized for my own needs.  I have recreated all teh figures although there are still questions, especially about the confidence intervals.

Note: in the Excel file - "RB_carlestrub_output_each_spp_by_site.xlsx", the estimates for all but **stand.species.biomass.contr** and abundance were hidden.  This made it look like they hadn't been done.  In the corresponding csv file, the columns have been relabelled **biomass_cs** and missing species have been added with zeros and the "Type", i.e., "Compensation" or "Main" has been written in.  But this means that I don't have to redo all the estimates and could use the Delta method.    


# Issues
## Delta Method (DM)
So, I have reconstructed the Delta Method in scratch pad using BT:2000 sites - see scratch_pad.R for the explanation.  Kristin did not use the DM as far as I know.  Usually, the DM results in a higher variance than naive averaging of the variances but not for BT:2000:Compensation.  
Note: The more replicates you have, the larger the denominator and ergo, the lower the variance.  
Note: its the sum of the partial derivatives * variance by site that gives the combined variance - I initially summed the partial derivates.  

So, the question is, 1) to use Kristin's values which are probably actually a bit of an overestimate or 2) to reconstruct everything from scratch with the AMEC functions and build the machinary that is in scratch_pad.R for everything.

I guess it comes down to the data sets used for the figures and the eventual analyses:  
- RB_carlestrub_output_each_spp_by_site -> this is distilled down to means and naive SEs by species and year; (graphs in mean_estimates_se_species.R)    
- RB_MeanBioComp/RB_meansalmonidsbysite  -> these have already been distilled down to compensation and main; (graphs in RB_figs.R)  (see next section).  I don't think that this is quite right - I think that this effectively groups all of the sites as one big site; this is the essence of pseudo-replication.


## what did Kristin do with the AMEC functions?  
- did she modify different input files, e.g., by site, main v compensation, low/medium/upper v main, by salmonids or different species, and feed them separately into the AMEC functions?  Or, did she modify the output??  ANSWER: I strongly suspect the later.  See scratch_pad.R - in "origin", I showed that running "RoseBlancheAug2000bytypecombinednoeels.csv" through the AMEC function perfectly reproduces the data in "RB_salmonids_maincomp.csv".  So, this means that the data were likely manipulated in Excel before hand and then fed in for custom summaries.  I think that "Station" is simply whatever the analyst wants to summarize the data on, kind of like group_by.

- zeros were added when there were no fish, presumably under the assumption that three passes with an electro fisher should result in at least some fish being caught.  
- NA's show up in the variance because there aren't enough fish to estimate it
Note: there are three possible scenarios: pt estimate == 0 and NA for CI, pt estiamte > 0 but NA for CI bc not enough samples to estiamte variance, pt estimate > 0 and variance > 0 [ this can all occurn in one species, year, e.g., RB:AS:2002:Main_Stem]  
- Variance will increase do to few samples and range of weight of fish.  
- CIs for BT were generally small at the Station level but for BT:Station7:2000-2001, there were either few fish caught or the assumption of catching fewer fish with each pass was violated.  This seems to result in the large confidence intervals.  

# Conclusions
I think that the best thing to do, ito graphs and analyses, is to redo this from the ground up.  Clearly, there is a lot of good work that was done but its not reproducible and if any reviewer knows about this stuff, we will have to redo it anyway.  The existing data sets have either been naively calculated or, just don't have the variance estimates (confidence intervals).

# Post 2025, August
I did a lot of work initially on Rose Blanche and this helped me to arrive at the approaches used on Pamehac and Granite.  
However, I abandoned it because I didn't have the lat/longs and the 1998 data.  **I have tried to locate the 1998 data to no avail.**  I does not appear to be on the archive, nor is it in Scruton's papers.  So, we'll just have to proceed with a CI study and hope that its OK.  

The below will follow the format of the Pamehac study.  See Pamehac and Granite for thoughts on models and approach and what have I learned.

# Data {.tabset}

## Notes
...

## Files
- RB_data_new.R: import data  
- RB_abun_bio.R: do FSA and goodness-of-fit calculations - RB_figs.R  
- RB_fun.R  
- RB_analyses.R: GLMMs  
- RB_tables: bootstrap values 

## Pipeline
**check the below**
SD = sweep disaggreated, SA = sweep aggregated
- ls_rb: list of electrofishing files in the Pamehac directory; SD  
- df_all: take ls_rb and make a dataframe; this includes **American Eels**; SD  
- df_sum: from df_all without **American Eel**, calculate biomass and abundance for each sweep; SD  
- df_grid: create a grid with all possible rows for Year, Station, Species, and Sweep - this is to take care of the "no line when no fish of a given species is caught" problem; SD  
df_grid1: take out the structural zeros; assume that 5 sweeps were conducted in 2000 but only 4 in later years.  Anything below the max for a year is a real zero, not a structural one; SD  
- df_all1: join df_grid1 with df_sum to make a complete data set, i.e., real zeros but no structural ones; SD  
- df_all2: join df_all1 with df_stn_tag (max sweeps per year) to have a max sweep column - not really used much; **replace NA's with zeros**; calculate spc (spc calculations are accurate); SD  
- df_a (object in R, add a number to csv files): Sweeps < 3; sum abundance and biomass; SA; df_a3: add area and standardize abund/bio; add location  
- df_tab1: tabular format of abundance and biomass, Sweep <= 3; SA  
- df_tab2: tabular format of abundance and biomass, all Sweeps; SA  
- df_tab_T: sum by species and year; tabular format


## map
```{r, map, echo = F}
tmp <- read.csv("../data/waypoints_RB.csv") |>
  dplyr::filter(!(Site == "rb8pp" |
             Site == "Rbcsixtpp" |
             Site == "Site 3"  |
             Site == "Site rbc4" |
             Site == "Rbmaim9tp")
         )
library(leaflet)
leaflet()%>%
   setView(lng = -58.7,lat=47.64,zoom=12)%>%addTiles()%>%
   addCircleMarkers(data=tmp, lng=tmp$west, lat=tmp$north,
                    popup = ~sites,
                    label = ~sites,
                    radius = 6,
                    color = 'black',
                    fillColor = 'blue',
                    stroke = TRUE,
                    weight = 1,
                    fillOpacity = 0.5) 

```


# Analyses
See restoration Granite Readme.qmd and Pamehac Readme.Rmd, especially the What Have I Learned (WHIL)

**need to check the below**

## Analyses {.tabset}

### data summaries
```{r}
nrow(df_all) # number of fish caught
sum(rowSums(!is.na(df_tab2[,4:8]))) # number of sites sampled assuming 5 passes in 2000,  and 4 in 2001,2002, and 2015, including sweeps with abun == 0
nrow(df_tab2) # number of site:species:years including sweeps with abun == 0; assumes previous is true
nrow(df_a) # number of site:species:years that can be used in FSA
nrow(df_out)
```

### low n
```{r}

nrow(df_a |> filter(abun < 30)) # 150 of 160
nrow(df_a |> filter(abun < 20)) # 135 of 160
nrow(df_a |> filter(abun < 10)) # 109 of 160
nrow(df_a |> filter(abun < 5)) # 79 of 160

nrow(df_out |> filter(T < 30)) # 150 of 160
nrow(df_out |> filter(T < 20)) # 135 of 160
nrow(df_out |> filter(T < 10)) # 109 of 160
nrow(df_out |> filter(T < 5)) # 79 of 160
```

### spc
```{r, echo=FALSE}
p <- ggplot(df_sum_subset, 
  aes(x = spc, y = abun, 
    group = Station, fill = Station,
    text = paste("SPC: ", spc, "\n",
                 "Abund: ", abun, "\n",
                 "Stn: ", Station, "\n",
                 "Sweep: ", Sweep,
                 sep = "")
  )) +
  geom_point() +
  geom_path()

plotly::ggplotly(p, tooltip = "text")
```

### GF

```{r, echo=FALSE}
print("critical value")
qchisq(0.95, 1)
print("count of year:species:sites")
nrow(df_a)
nrow(df_out) # 160
nrow(df_out |> filter(GF < qchisq(0.95, 1))) # 90 of 160 (56%)
nrow(df_out |> filter(GF >= qchisq(0.95, 1))) # 17 of 160 (11%)
nrow(df_out |> filter(is.na(GF))) # 53 of 160 (33%)

```

### FSA: T v No


```{r, echo=FALSE}
ggplot(df_out, aes(x = T, y = No, group = as.factor(spp), colour = spp)) +
  geom_point()
```

### sites with 4-5 passes and fish

```{r, echo=F}
df_all |>
  group_by(Year, Species, Station) |>
  mutate(pass_no = ifelse(max(Sweep )<=3, 3, 5)) |>
  ungroup() |>
  filter(pass_no == 5) |> 
  group_by(Year, Species, Station, Sweep) |>
  summarize(count = n()) |>
  pivot_wider(names_from = Sweep, values_from = count, values_fill = 0) |>
  mutate(percent4_5 = (sum(`4`, `5`)/sum(`1`, `2`, `3`)*100), 
         percent4 = (sum(`4`)/sum(`1`, `2`, `3`)*100),
         diff = sum(`1`, `2`, `3`) - sum(`1`, `2`, `3`, `4`)) |>
  relocate(`1`, .after = Station) |>
  relocate(`4`, .after = `3`)

```

### p v T
```{r, echo=FALSE}
n <- 100
p <- 0.5

c1 <- p*n
c2 <- (n-c1)*p
c3 <- (n- c1-c2)*p
T <- sum(c1, c2, c3)

p_vec <- seq(0.01, 1, 0.01)
T_vec <- rep(NA, length(p_vec))

i <- 2
for(i in seq_along(p_vec)){
  c1 <- p_vec[i]*n
  c2 <- (n-c1)*p_vec[i]
  c3 <- (n- c1-c2)*p_vec[i]
  T_vec[i] <- sum(c1, c2, c3)
}

plot(p_vec, T_vec, type = 'n')
lines(p_vec, T_vec)
abline(h=90)
```

### Summary/conclusions
- Lots of very low catches, very few over 30  
- Lots of sites with GF > GF crit  
- Few fish caught after 4-5 passes

** Go with T**

## Presentation

Aug 26, 2025: had a bit of a dilemma about how to present the results given that my thinking has evolved over time and I did things somewhat differently (or very differently for each of the 4 study areas).  My dilemma revolved around how to best present the parameter estimates (i.e., NHST type info) and yearly estimates for the response variable.

Response variable: For Seal Cove (including the paper), I presented the density/biomass as naive point estimates with standard errors, i.e,. I just took the mean and se of the stations for a particular time/treatment for each year.  This isn't awful but when the point estimate is low and there is substantial variance, the CI can be very wide and even include zero.  This is especially true for Rose Blanche where there are many low counts.  I also considered using predicted values but i'm concerned with the effect that zi and dispersion model have and that i'm not properly taking these into account.  Therefore, I have concluded that the best approach is to use the bootstrp to calculate density/biomass estimates for Species, Year, and Type which I will present in figures but with tables in an appendix (currently in dashboard).  This will be more consistent/certain and also better reflect the uncertainty in the estimates.

Parameter estimates: For Seal Cove, I presented the parameter estimates of the conditional models in a table.  But I had thought it might be good to do this with the predicted values but in a figure.  However, I decided to abandon the graphical presentation of the predicted values which I had done for Pamehac.  I didn't do anything wrong here - this approach is not a bad thing but they only "appeared" to work for Pamehac because the conditional models were just interaction plus random effect.  The Rose Blanche models often had other terms to help out the diagnostics.  This means that the predicted values are different for each Station within a Year for the latter i.e., a different value for each station per level but not the former, i.e, the same value for each station per level.  Note though that the control_impact_year function effectively shows the predicted values for each station by year and this might be good to remember.



# References
